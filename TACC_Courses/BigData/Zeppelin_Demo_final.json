{"paragraphs":[{"text":"%sh echo $SPARK_HOME","user":"user2181","dateUpdated":"2017-04-18T15:01:31-0500","config":{"colWidth":12,"editorMode":"ace/mode/sh","results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false}},"settings":{"params":{"SPARK_HOME":""},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"/data/apps/spark-2.1.0-bin-hadoop2.7/\n"}]},"apps":[],"jobName":"paragraph_1492461637556_1450145538","id":"20170414-101356_221341498","dateCreated":"2017-04-17T15:40:37-0500","dateStarted":"2017-04-18T15:01:31-0500","dateFinished":"2017-04-18T15:01:31-0500","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3967"},{"text":"%sh spark-submit --version\n","user":"user2181","dateUpdated":"2017-04-18T15:01:33-0500","config":{"colWidth":12,"editorMode":"ace/mode/sh","results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Welcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.1.0\n      /_/\n                        \nUsing Scala version 2.11.8, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_92\nBranch \nCompiled by user jenkins on 2016-12-16T02:04:48Z\nRevision \nUrl \nType --help for more information.\n"}]},"apps":[],"jobName":"paragraph_1492461637573_1431292842","id":"20170414-101550_306140230","dateCreated":"2017-04-17T15:40:37-0500","dateStarted":"2017-04-18T15:01:34-0500","dateFinished":"2017-04-18T15:01:38-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3968"},{"text":"%sh yarn node -list\n","user":"user2181","dateUpdated":"2017-04-18T16:35:16-0500","config":{"colWidth":12,"editorMode":"ace/mode/sh","results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"17/04/18 16:35:20 INFO client.RMProxy: Connecting to ResourceManager at c252-109.wrangler.tacc.utexas.edu/129.114.58.152:8032\nTotal Nodes:3\n         Node-Id\t     Node-State\tNode-Http-Address\tNumber-of-Running-Containers\nc252-112.wrangler.tacc.utexas.edu:57753\t        RUNNING\tc252-112.wrangler.tacc.utexas.edu:8042\t                           0\nc252-110.wrangler.tacc.utexas.edu:44787\t        RUNNING\tc252-110.wrangler.tacc.utexas.edu:8042\t                           0\nc252-111.wrangler.tacc.utexas.edu:44124\t        RUNNING\tc252-111.wrangler.tacc.utexas.edu:8042\t                           0\n"}]},"apps":[],"jobName":"paragraph_1492461637577_1429753846","id":"20170414-102331_719338610","dateCreated":"2017-04-17T15:40:37-0500","dateStarted":"2017-04-18T16:35:16-0500","dateFinished":"2017-04-18T16:35:23-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3969"},{"text":"%sh \nhadoop fs -put /work/00791/xwj/DMS/hadoop-training/stopwords.txt .\nhadoop fs -put /work/00791/xwj/DMS/hadoop-training/book.txt .\n\nhadoop fs -ls\n\n","user":"user2181","dateUpdated":"2017-04-19T15:52:58-0500","config":{"colWidth":12,"editorMode":"ace/mode/sh","results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"put: `stopwords.txt': File exists\nput: `book.txt': File exists\nFound 5 items\ndrwxr-xr-x   - rhuang hadoop          0 2017-04-19 14:30 .sparkStaging\n-rw-r--r--   2 rhuang hadoop     400115 2017-04-15 11:14 book.txt\ndrwxr-xr-x   - rhuang hadoop          0 2017-04-19 15:03 data\ndrwxr-xr-x   - rhuang hadoop          0 2017-04-19 15:31 output-streaming-py\n-rw-r--r--   2 rhuang hadoop       1914 2017-04-15 11:14 stopwords.txt\n"}]},"apps":[],"jobName":"paragraph_1492461637581_1428214850","id":"20170414-102516_316806737","dateCreated":"2017-04-17T15:40:37-0500","dateStarted":"2017-04-19T15:52:58-0500","dateFinished":"2017-04-19T15:53:28-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3970"},{"text":"%spark\nimport org.apache.spark.rdd.RDD\nval textFile = sc.textFile(\"book.txt\")\n","user":"user2181","dateUpdated":"2017-04-18T15:05:57-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.rdd.RDD\n\ntextFile: org.apache.spark.rdd.RDD[String] = book.txt MapPartitionsRDD[98] at textFile at <console>:28\n"}]},"apps":[],"jobName":"paragraph_1492461637586_1440142066","id":"20170414-103614_1202371380","dateCreated":"2017-04-17T15:40:37-0500","dateStarted":"2017-04-18T15:05:58-0500","dateFinished":"2017-04-18T15:06:00-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3971"},{"text":"%sh yarn node -list","user":"user2181","dateUpdated":"2017-04-18T15:06:42-0500","config":{"colWidth":12,"editorMode":"ace/mode/sh","results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"17/04/18 15:06:47 INFO client.RMProxy: Connecting to ResourceManager at c252-109.wrangler.tacc.utexas.edu/129.114.58.152:8032\nTotal Nodes:3\n         Node-Id\t     Node-State\tNode-Http-Address\tNumber-of-Running-Containers\nc252-112.wrangler.tacc.utexas.edu:57753\t        RUNNING\tc252-112.wrangler.tacc.utexas.edu:8042\t                           4\nc252-110.wrangler.tacc.utexas.edu:44787\t        RUNNING\tc252-110.wrangler.tacc.utexas.edu:8042\t                           5\nc252-111.wrangler.tacc.utexas.edu:44124\t        RUNNING\tc252-111.wrangler.tacc.utexas.edu:8042\t                           4\n"}]},"apps":[],"jobName":"paragraph_1492461637589_1437448824","id":"20170415-111537_1786415762","dateCreated":"2017-04-17T15:40:37-0500","dateStarted":"2017-04-18T15:06:43-0500","dateFinished":"2017-04-18T15:06:50-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3972"},{"text":"textFile.count() // Number of items in this RDD%spark\n","user":"user2181","dateUpdated":"2017-04-18T15:06:57-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nres0: Long = 7454\n"}]},"apps":[],"jobName":"paragraph_1492461637593_1435909828","id":"20170414-103717_443112498","dateCreated":"2017-04-17T15:40:37-0500","dateStarted":"2017-04-18T15:06:58-0500","dateFinished":"2017-04-18T15:07:00-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3973"},{"text":"textFile.first() // First item in this RDD","user":"user2181","dateUpdated":"2017-04-18T15:07:02-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nres1: String = The Project Gutenberg EBook of The Hand of Providence, by J. H. Ward\n"}]},"apps":[],"jobName":"paragraph_1492461637596_1434755582","id":"20170414-103739_730204183","dateCreated":"2017-04-17T15:40:37-0500","dateStarted":"2017-04-18T15:07:02-0500","dateFinished":"2017-04-18T15:07:04-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3974"},{"text":"textFile.filter(line => line.contains(\"Providence\")).count() ","user":"user2181","dateUpdated":"2017-04-18T15:07:07-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nres2: Long = 16\n"}]},"apps":[],"jobName":"paragraph_1492461637600_1420904621","id":"20170414-103958_462401596","dateCreated":"2017-04-17T15:40:37-0500","dateStarted":"2017-04-18T15:07:07-0500","dateFinished":"2017-04-18T15:07:09-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3975"},{"text":"val stopWords = sc.textFile(\"stopwords.txt\")\nval stopWordSet = stopWords.collect.toSet\nval stopWordSetBC = sc.broadcast(stopWordSet)\n\nimport org.apache.spark.sql.Row\n//textFile.flatMap(_.toLowerCase.split(\" \")).subtract(stopWords).take(100)\nval wordCounts =  textFile.flatMap(_.toLowerCase.split(\" \")).filter( w => !stopWordSetBC.value.contains(w)&&w!=\"\").map(word => (word, 1)).reduceByKey((a, b) => a + b)\n \nval top50 = wordCounts.sortBy(_._2,ascending=false).map{case (word:String,count:Int) => {word + \"\\t\" + count} }.take(50)\n//top50.mkString(\"\\n\")\nprint(\"%table Word\\t Count\\n\" + top50.mkString(\"\\n\"))\n\n\n","user":"user2181","dateUpdated":"2017-04-18T15:07:11-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{"1":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false},"helium":{}}},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nstopWords: org.apache.spark.rdd.RDD[String] = stopwords.txt MapPartitionsRDD[101] at textFile at <console>:28\nstopWordSet: scala.collection.immutable.Set[String] = Set(serious, latterly, down, side, moreover, please, ourselves, behind, for, find, further, mill, due, any, wherein, across, twenty, name, this, in, move, itse\", have, your, off, once, are, is, his, why, too, among, everyone, show, empty, already, nobody, less, am, hence, system, than, four, fire, anyhow, three, whereby, con, twelve, throughout, but, whether, below, co, mine, becomes, eleven, what, would, although, elsewhere, another, front, if, hereby, own, neither, bottom, up, etc, so, our, per, therein, must, beforehand, keep, do, all, him, had, somehow, re, onto, nor, every, herein, full, before, afterwards, somewhere, whither, else, namely, us, it, whereupon, two, thence, a, herse\", sometimes, became, though, within, as, because...\nstopWordSetBC: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Set[String]] = Broadcast(46)\n\nimport org.apache.spark.sql.Row\n\nwordCounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[105] at reduceByKey at <console>:37\n\ntop50: Array[String] = Array(great\t165, men\t110, time\t101, years\t95, new\t92, people\t87, project\t85, thousand\t75, work\t65, god\t65, history\t63, england\t62, ancient\t62, religious\t61, long\t58, city\t57, human\t56, gutenberg-tm\t54, man\t54, english\t53, american\t52, world\t52, king\t50, old\t48, came\t45, europe\t45, chapter\t44, took\t43, did\t43, called\t42, roman\t42, length\t41, -\t40, day\t40, life\t40, nations\t40, gave\t39, nearly\t39, world.\t39, a.\t38, d.\t38, days\t37, received\t37, hand\t37, little\t37, vast\t37, place\t36, british\t36, [image:\t36, modern\t35)\n"},{"type":"TABLE","data":"Word\t Count\ngreat\t165\nmen\t110\ntime\t101\nyears\t95\nnew\t92\npeople\t87\nproject\t85\nthousand\t75\nwork\t65\ngod\t65\nhistory\t63\nengland\t62\nancient\t62\nreligious\t61\nlong\t58\ncity\t57\nhuman\t56\ngutenberg-tm\t54\nman\t54\nenglish\t53\namerican\t52\nworld\t52\nking\t50\nold\t48\ncame\t45\neurope\t45\nchapter\t44\ntook\t43\ndid\t43\ncalled\t42\nroman\t42\nlength\t41\n-\t40\nday\t40\nlife\t40\nnations\t40\ngave\t39\nnearly\t39\nworld.\t39\na.\t38\nd.\t38\ndays\t37\nreceived\t37\nhand\t37\nlittle\t37\nvast\t37\nplace\t36\nbritish\t36\n[image:\t36\nmodern\t35"}]},"apps":[],"jobName":"paragraph_1492461637603_1421289370","id":"20170414-110223_612269431","dateCreated":"2017-04-17T15:40:37-0500","dateStarted":"2017-04-18T15:07:11-0500","dateFinished":"2017-04-18T15:07:36-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3976"},{"text":"%pyspark\ntextFile = sc.textFile(\"book.txt\")\n\nstopWords = sc.textFile(\"stopwords.txt\")\n\n\nwordCounts = textFile.flatMap(lambda line: line.lower().split()).subtract(stopWords).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)\ntop50 = wordCounts.sortBy(lambda a: a[1],ascending=False).map(lambda x: x[0]+ \"\\t\" + str(x[1])).take(50)\n\nprint(\"%table Word\\t Count\\n\" + \"\\n\".join(top50))","user":"user2181","dateUpdated":"2017-04-19T14:37:14-0500","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{"0":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"setting":{"multiBarChart":{"stacked":false}},"commonSetting":{},"keys":[{"name":"Word","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":" Count","index":1,"aggr":"sum"}]},"helium":{}}},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"Word\t Count\ngreat\t165\nmen\t110\ntime\t101\nyears\t95\nnew\t92\npeople\t87\nproject\t85\nthousand\t75\ngod\t65\nwork\t65\nhistory\t63\nancient\t62\nengland\t62\nreligious\t61\nlong\t58\ncity\t57\nhuman\t56\ngutenberg-tm\t54\nman\t54\nenglish\t53\nworld\t52\namerican\t52\nking\t50\nold\t48\ncame\t45\neurope\t45\nchapter\t44\ntook\t43\ndid\t43\ncalled\t42\nroman\t42\nlength\t41\nday\t40\nnations\t40\nlife\t40\n-\t40\nnearly\t39\ngave\t39\nworld.\t39\na.\t38\nd.\t38\ndays\t37\nreceived\t37\nhand\t37\nvast\t37\nlittle\t37\n[image:\t36\nbritish\t36\nplace\t36\nmodern\t35\n"}]},"apps":[],"jobName":"paragraph_1492461637609_1417441881","id":"20170414-142326_1866834986","dateCreated":"2017-04-17T15:40:37-0500","dateStarted":"2017-04-19T14:29:39-0500","dateFinished":"2017-04-19T14:33:16-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3977"},{"text":"%sh \nmodule list","user":"user2181","dateUpdated":"2017-04-18T15:08:03-0500","config":{"colWidth":12,"editorMode":"ace/mode/sh","results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nCurrently Loaded Modules:\n  1) TACC-paths      4) intel/15.0.3   7) TACC\n  2) Linux           5) mvapich2/2.1   8) RstatsPackages/3.2.1\n  3) cluster-paths   6) cluster        9) Rstats/3.2.1\n\n \n\n"}]},"apps":[],"jobName":"paragraph_1492461637613_1415902886","id":"20170414-145612_567229201","dateCreated":"2017-04-17T15:40:37-0500","dateStarted":"2017-04-18T15:08:03-0500","dateFinished":"2017-04-18T15:08:04-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3978"},{"text":"%sh\necho $LD_LIBRARY_PATH\n# copy and paste the below and set spark.executor.extraLibraryPath in the interpreter spark setting","user":"user2181","dateUpdated":"2017-04-19T15:39:32-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"/opt/apps/intel15/mvapich2_2_1/RstatsPackages/3.2.1/jags-3.4.0/lib64/JAGS/modules-3:/opt/apps/intel15/mvapich2_2_1/RstatsPackages/3.2.1/jags-3.4.0/lib64:/opt/apps/intel15/mvapich2_2_1/RstatsPackages/3.2.1/proj-4.7.0/lib:/opt/apps/intel15/mvapich2_2_1/RstatsPackages/3.2.1/protobuf-4.7.1/lib:/opt/apps/intel15/mvapich2_2_1/RstatsPackages/3.2.1/gdal-1.9.2/lib:/opt/apps/intel15/mvapich2_2_1/RstatsPackages/3.2.1/nlopt-2.4.2/lib:/opt/apps/gcc/4.9.1/lib64:/opt/apps/gcc/4.9.1/lib:/opt/apps/intel15/mvapich2_2_1/Rstats/3.2.1/lib64/R/lib:/opt/apps/intel15/mvapich2/2.1/lib:/opt/apps/intel15/mvapich2/2.1/lib/shared:/opt/apps/intel/15/composer_xe_2015.3.187/mpirt/lib/intel64:/opt/apps/intel/15/composer_xe_2015.3.187/ipp/lib/intel64:/opt/apps/intel/15/composer_xe_2015.3.187/mkl/lib/intel64:/opt/apps/intel/15/composer_xe_2015.3.187/tbb/lib/intel64:/opt/apps/intel/15/composer_xe_2015.3.187/tbb/lib/intel64/gcc4.4:/opt/apps/intel/15/composer_xe_2015.3.187/compiler/lib/intel64\n"}]},"apps":[],"jobName":"paragraph_1492633966106_-1154066719","id":"20170419-153246_1594249797","dateCreated":"2017-04-19T15:32:46-0500","dateStarted":"2017-04-19T15:33:38-0500","dateFinished":"2017-04-19T15:33:38-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3979"},{"text":"%spark.r\n\n#detach(\"package:dplyr\", unload=TRUE)\n\npeople <- read.df(sprintf(\"file:%s/examples/src/main/resources/people.json\",Sys.getenv('SPARK_HOME')), \"json\")\nhead(people)\nprintSchema(people)\n\n# From Hive tables\nsql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING)\")\ninput = sprintf(\"file:%s/examples/src/main/resources/kv1.txt\",Sys.getenv('SPARK_HOME'))\nsql(sprintf(\"LOAD DATA LOCAL INPATH '%s' INTO TABLE src\",input))\n# Queries can be expressed in HiveQL.\nresults <- sql(\"FROM src SELECT key, value\")\nprintSchema(results)\n# results is now a SparkDataFrame\ndim(results)\nresults\n\nschema <- structType(structField(\"key\", \"integer\"), structField(\"value\", \"string\"),\n                     structField(\"key2\", \"double\"))\n                     \ndf1 <- dapply(results, function(x) { x <- cbind(x, x$key * 2) }, schema)\nhead(df1)\n\n\n\n","user":"user2181","dateUpdated":"2017-04-18T15:08:06-0500","config":{"colWidth":12,"editorMode":"ace/mode/r","results":{},"enabled":true,"editorSetting":{"language":"r","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nage    name\n1  NA Michael\n2  30    Andy\n3  19  Justin\nroot\n |– age: long (nullable = true)\n |– name: string (nullable = true)\nSparkDataFrame[]\nSparkDataFrame[]\nroot\n |– key: integer (nullable = true)\n |– value: string (nullable = true)\n[1] 26000     2\nSparkDataFrame[key:int, value:string]\n  key   value key2\n1 238 val_238  476\n2  86  val_86  172\n3 311 val_311  622\n4  27  val_27   54\n5 165 val_165  330\n6 409 val_409  818\n\n\n\n"}]},"apps":[],"jobName":"paragraph_1492461637618_1427830102","id":"20170414-164622_908777772","dateCreated":"2017-04-17T15:40:37-0500","dateStarted":"2017-04-18T15:08:06-0500","dateFinished":"2017-04-18T15:08:16-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3980"},{"text":"%spark.r\n### Running SQL Queries from SparkR\npeople <- read.df(sprintf(\"file:%s/examples/src/main/resources/people.json\",Sys.getenv('SPARK_HOME')), \"json\")\nhead(people)\n\n# Register this SparkDataFrame as a temporary view.\ncreateOrReplaceTempView(people, \"people\")\n\n# SQL statements can be run by using the sql method\nteenagers <- sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\nhead(teenagers)\n","user":"user2181","dateUpdated":"2017-04-18T15:08:19-0500","config":{"colWidth":12,"editorMode":"ace/mode/r","results":{},"enabled":true,"editorSetting":{"language":"r","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nage    name\n1  NA Michael\n2  30    Andy\n3  19  Justin\n    name\n1 Justin\n\n\n\n"}]},"apps":[],"jobName":"paragraph_1492461637621_1425136859","id":"20170415-162418_1949098877","dateCreated":"2017-04-17T15:40:37-0500","dateStarted":"2017-04-18T15:08:20-0500","dateFinished":"2017-04-18T15:08:22-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3981"},{"text":"%spark.r\n# Create the SparkDataFrame\ndf <- as.DataFrame(faithful)\n\n# Get basic information about the SparkDataFrame\ndf\ndim(df)\n\n# Select only the \"eruptions\" column\nhead(select(df, df$eruptions))\n\n# You can also pass in column name as strings\nhead(select(df, \"eruptions\"))\n\n# Filter the SparkDataFrame to only retain rows with wait times shorter than 50 mins\nhead(filter(df, df$waiting < 50))","user":"user2181","dateUpdated":"2017-04-18T15:08:25-0500","config":{"colWidth":12,"editorMode":"ace/mode/r","results":{},"enabled":true,"editorSetting":{"language":"r"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nSparkDataFrame[eruptions:double, waiting:double]\n[1] 272   2\n  eruptions\n1     3.600\n2     1.800\n3     3.333\n4     2.283\n5     4.533\n6     2.883\n  eruptions\n1     3.600\n2     1.800\n3     3.333\n4     2.283\n5     4.533\n6     2.883\n  eruptions waiting\n1     1.750      47\n2     1.750      47\n3     1.867      48\n4     1.750      48\n5     2.167      48\n6     2.100      49\n\n\n\n"}]},"apps":[],"jobName":"paragraph_1492461637625_1423597864","id":"20170415-162005_1073880098","dateCreated":"2017-04-17T15:40:37-0500","dateStarted":"2017-04-18T15:08:26-0500","dateFinished":"2017-04-18T15:08:37-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3982"},{"text":"%spark.r\n# We use the `n` operator to count the number of times each waiting time appears\nhead(summarize(groupBy(df, df$waiting), count = n(df$waiting)))\n\n# We can also sort the output from the aggregation to get the most common waiting times\nwaiting_counts <- summarize(groupBy(df, df$waiting), count = n(df$waiting))\nhead(arrange(waiting_counts, desc(waiting_counts$count)))\n\n","user":"user2181","dateUpdated":"2017-04-18T15:08:43-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"r"},"editorMode":"ace/mode/r"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nwaiting count\n1      70     4\n2      67     1\n3      69     2\n4      88     6\n5      49     5\n6      64     4\n  waiting count\n1      78    15\n2      83    14\n3      81    13\n4      77    12\n5      82    12\n6      79    10\n\n\n\n"}]},"apps":[],"jobName":"paragraph_1492537842380_-1970432132","id":"20170418-125042_1634861141","dateCreated":"2017-04-18T12:50:42-0500","dateStarted":"2017-04-18T15:08:43-0500","dateFinished":"2017-04-18T15:08:54-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3983"},{"text":"%spark.r\n##### Run a given function on a large dataset using dapply or dapplyCollect\n# Convert waiting time from hours to seconds.\n# Note that we can apply UDF to DataFrame.\nschema <- structType(structField(\"eruptions\", \"double\"), structField(\"waiting\", \"double\"),\n                     structField(\"waiting_secs\", \"double\"))\ndf1 <- dapply(df, function(x) { x <- cbind(x, x$waiting * 60) }, schema)\nhead(collect(df1))\n","user":"user2181","dateUpdated":"2017-04-18T15:08:58-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"r"},"editorMode":"ace/mode/r"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\neruptions waiting waiting_secs\n1     3.600      79         4740\n2     1.800      54         3240\n3     3.333      74         4440\n4     2.283      62         3720\n5     4.533      85         5100\n6     2.883      55         3300\n\n\n\n"}]},"apps":[],"jobName":"paragraph_1492542477266_-2116016122","id":"20170418-140757_1747507733","dateCreated":"2017-04-18T14:07:57-0500","dateStarted":"2017-04-18T15:08:58-0500","dateFinished":"2017-04-18T15:09:00-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3984"},{"text":"%spark.r\n#### Applying User-Defined Function\n#### Run a given function on a large dataset grouping by input column(s) and using gapply or gapplyCollect\n# Determine six waiting times with the largest eruption time in minutes.\nschema <- structType(structField(\"waiting\", \"double\"), structField(\"max_eruption\", \"double\"))\nresult <- gapply(\n    df,\n    \"waiting\",\n    function(key, x) {\n        y <- data.frame(key, max(x$eruptions))\n    },\n    schema)\nhead(collect(arrange(result, \"max_eruption\", decreasing = TRUE)))\n","user":"user2181","dateUpdated":"2017-04-18T15:09:10-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"r"},"editorMode":"ace/mode/r"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nwaiting max_eruption\n1      96        5.100\n2      76        5.067\n3      77        5.033\n4      88        5.000\n5      86        4.933\n6      82        4.900\n\n\n\n"}]},"apps":[],"jobName":"paragraph_1492543468924_183439754","id":"20170418-142428_2077564295","dateCreated":"2017-04-18T14:24:28-0500","dateStarted":"2017-04-18T15:09:10-0500","dateFinished":"2017-04-18T15:09:16-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3985"},{"text":"%spark.r\n### Run local R functions distributed using spark.lapply\n# Perform distributed training of multiple models with spark.lapply. Here, we pass\n# a read-only list of arguments which specifies family the generalized linear model should be.\nfamilies <- c(\"gaussian\", \"poisson\")\ntrain <- function(family) {\n  model <- glm(Sepal.Length ~ Sepal.Width + Species, iris, family = family)\n  summary(model)\n}\n# Return a list of model's summaries\nmodel.summaries <- spark.lapply(families, train)\n\n# Print the summary of each model\nprint(model.summaries)\n","user":"user2181","dateUpdated":"2017-04-18T15:09:19-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"r"},"editorMode":"ace/mode/r"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\n[[1]]\n\nCall:\nglm(formula = Sepal.Length ~ Sepal.Width + Species, family = family, \n    data = iris)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max\n-1.30711  -0.25713  -0.05325   0.19542   1.41253  \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)         2.2514     0.3698   6.089 9.57e-09 ***\nSepal.Width         0.8036     0.1063   7.557 4.19e-12 ***\nSpeciesversicolor   1.4587     0.1121  13.012  &lt; 2e-16 ***\n\nSpeciesvirginica    1.9468     0.1000  19.465  &lt; 2e-16 ***\n\nSignif. codes:  0 '**' 0.001 '' 0.01 '' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.1918059)\n\nNull deviance: 102.168  on 149  degrees of freedom\n\n\nResidual deviance:  28.004  on 146  degrees of freedom\nAIC: 183.94\n\nNumber of Fisher Scoring iterations: 2\n\n[[2]]\n\nCall:\nglm(formula = Sepal.Length ~ Sepal.Width + Species, family = family, \n    data = iris)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max\n-0.52652  -0.10966  -0.01230   0.07755   0.56101  \n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)        1.13033    0.35454   3.188 0.001432 ** \nSepal.Width        0.13971    0.10119   1.381 0.167361\nSpeciesversicolor  0.26277    0.10901   2.410 0.015931 *  \n\nSpeciesvirginica   0.33842    0.09587   3.530 0.000416 ***\n\nSignif. codes:  0 '**' 0.001 '' 0.01 '' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\nNull deviance: 17.3620  on 149  degrees of freedom\n\n\nResidual deviance:  4.5202  on 146  degrees of freedom\nAIC: Inf\n\nNumber of Fisher Scoring iterations: 3\n\n\n\n"}]},"apps":[],"jobName":"paragraph_1492543633465_-505247098","id":"20170418-142713_479684775","dateCreated":"2017-04-18T14:27:13-0500","dateStarted":"2017-04-18T15:09:19-0500","dateFinished":"2017-04-18T15:09:20-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3986"},{"text":"%sh\n# run python work count hadoop example\ncd /work/00791/xwj/DMS/hadoop-training/hadoop-streaming-py/\nhadoop fs -mkdir data\nhadoop fs -put /data/03076/rhuang/training_dataset/book.txt data\nhadoop fs -rm -r  output-streaming-py\nexport HADOOP_STREAMING=/usr/lib/hadoop-mapreduce/hadoop-streaming.jar \nsource wordcount.sh\n# the last two lines of errors message not important, success anyway\n\n","user":"user2181","dateUpdated":"2017-04-19T15:35:08-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"INCOMPLETE","msg":[{"type":"TEXT","data":"mkdir: `data': File exists\nput: `data/book.txt': File exists\nDeleted output-streaming-py\n17/04/19 15:29:53 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\npackageJobJar: [mapper.py, reducer.py] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.2.jar] /tmp/streamjob7307957412870837712.jar tmpDir=null\n17/04/19 15:29:58 INFO client.RMProxy: Connecting to ResourceManager at c252-109.wrangler.tacc.utexas.edu/129.114.58.152:8032\n17/04/19 15:29:59 INFO client.RMProxy: Connecting to ResourceManager at c252-109.wrangler.tacc.utexas.edu/129.114.58.152:8032\n17/04/19 15:30:03 INFO mapred.FileInputFormat: Total input paths to process : 1\n17/04/19 15:30:03 INFO mapreduce.JobSubmitter: number of splits:4\n17/04/19 15:30:03 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n17/04/19 15:30:03 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n17/04/19 15:30:03 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1492270400644_0035\n17/04/19 15:30:04 INFO impl.YarnClientImpl: Submitted application application_1492270400644_0035\n17/04/19 15:30:04 INFO mapreduce.Job: The url to track the job: http://c252-109.wrangler.tacc.utexas.edu:8088/proxy/application_1492270400644_0035/\n17/04/19 15:30:05 INFO mapreduce.Job: Running job: job_1492270400644_0035\n"},{"type":"TEXT","data":"Paragraph received a SIGTERM\nExitValue: 143"}]},"apps":[],"jobName":"paragraph_1492543703854_-1598010595","id":"20170418-142823_2030162085","dateCreated":"2017-04-18T14:28:23-0500","dateStarted":"2017-04-19T15:29:20-0500","dateFinished":"2017-04-19T15:31:09-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3987"},{"text":"%sh\nhadoop fs -cat output-streaming-py/*|head -n 20","user":"user2181","dateUpdated":"2017-04-19T15:31:57-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\"A\t3\n\"Araby\t1\n\"Beyond\t1\n\"But\t2\n\"By\t1\n\"Chastity,\"\t1\n\"Clothe\t1\n\"Common\t1\n\"Do\t1\n\"Does\t1\n\"For\t2\n\"Given\t1\n\"Golden\t1\n\"Great\t2\n\"Humble\t1\n\"I\t8\n\"Impart\t1\n\"In\t2\n\"It\t3\n\"Journal\t1\ncat: Unable to write to output stream.\ncat: Unable to write to output stream.\n"}]},"apps":[],"jobName":"paragraph_1492632523150_1896734313","id":"20170419-150843_1362801069","dateCreated":"2017-04-19T15:08:43-0500","dateStarted":"2017-04-19T15:31:57-0500","dateFinished":"2017-04-19T15:32:07-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3988"},{"text":"%sh\n","user":"user2181","dateUpdated":"2017-04-19T15:10:02-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1492632602224_176661543","id":"20170419-151002_66646991","dateCreated":"2017-04-19T15:10:02-0500","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:3989"}],"name":"Zeppelin_Demo_final","id":"2CFFFHM8F","angularObjects":{"2CDYJDJYJ:shared_process":[],"2CGNHMN3B:shared_process":[],"2CEEMSSX4:shared_process":[],"2CERNVFUC:shared_process":[],"2CFSTE4S8:shared_process":[],"2CDREAV79:shared_process":[],"2CEHKKDEK:shared_process":[],"2CDA15AM4:shared_process":[],"2CDMBKDYB:shared_process":[],"2CG17Z45J:shared_process":[],"2CGSGMD2N:shared_process":[],"2CGGNG3KZ:shared_process":[],"2CFXMVDK2:shared_process":[],"2CEX45YU8:shared_process":[],"2CDD17AF4:shared_process":[],"2CF12XXEM:shared_process":[],"2CFNHNZXU:shared_process":[],"2CEY2BFED:shared_process":[],"2CFR4YW7W:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}