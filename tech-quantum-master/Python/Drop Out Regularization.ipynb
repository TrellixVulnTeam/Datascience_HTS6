{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Out Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now proceed to implement forward and backward propagation with drop out regularization\n",
    "\n",
    "\n",
    "Our simple neural network will have one input layer, one hidden layer with a tanh activation and an output layer with a sigmoid activation. We choose tanh cos its derivative is easy to caculate. \n",
    "\n",
    "To begin, we implement forward propagation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop_dropout(n_h, n_f, n_O, X, keep_prob):\n",
    "    \n",
    "    '''\n",
    "    Description: This funcition performs forward propagation with drop out in mind.\n",
    "    \n",
    "    input - \n",
    "    n_h - number of nodes in hidden layers\n",
    "    n_O - number of nodes in output layers\n",
    "    n_f - number of features in input layer\n",
    "    X - training samples\n",
    "    keep_prob - keep probability \n",
    "    \n",
    "    Output - \n",
    "    \n",
    "    forward - the results from the forward prop\n",
    "    parameters - netowrk parameters\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # We start by initializing the weights and biases\n",
    "    W1 = np.random.randn(n_h, n_f) * 0.1,\n",
    "    b1 = np.zeros((n_h, 1)),\n",
    "    W2 = np.random.randn(n_O, n_h) * 0.1,\n",
    "    b2 = np.zeros((n_O, 1))\n",
    "    \n",
    "    #Next, we perform forward propagation for the layers. \n",
    "    \n",
    "    # First, linear calculation for hidden layer\n",
    "    Z1 = np.dot(W1.T, X)  + b1\n",
    "    \n",
    "    # Activation for hidden layer\n",
    "    A = np.tanh(Z1)\n",
    "    \n",
    "    # Get dropout for hidden layer withthe same shape as activation output\n",
    "    D = np.random.rand(A.shape[0], A.shape[1])\n",
    "    \n",
    "    #Convert D to 1s or 0s based on the keep probability \n",
    "    D = D < keep_prob\n",
    "    \n",
    "    #Multiply A with D so as to randomly drop some nodes\n",
    "    A * D\n",
    "    \n",
    "    #Divide A by keep_prob to scale the number of neurons that have not been dropped\n",
    "    A = A /  keep_prob\n",
    "    \n",
    "    # Linear calculation for output layer\n",
    "    Z2 = np.dot(W2.T, X) + b2\n",
    "    \n",
    "    # Activation for output layer\n",
    "    Y_out = 1 / (1 + (np.exp(-Z2)))\n",
    "    \n",
    "    #Save initialized weights\n",
    "    Parameters = {\n",
    "        'W1': W1,\n",
    "        'b1': b1,\n",
    "        'W2': W2,\n",
    "        'b2': b2\n",
    "        \n",
    "    }\n",
    "    \n",
    "    # get results from forward propagation\n",
    "    forward = {\"Z1\": Z1,\n",
    "                \"A\": A,\n",
    "                \"Z2\": Z2,\n",
    "                \"Y_out\": Y_out,\n",
    "                \"D\" : D\n",
    "              \n",
    "              }\n",
    "    \n",
    "    return parameters, forwards\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now prceed to perform backward propagation with drop out regularization in mind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, get results from forward propagation by running the code below with numbers in place of the arguments\n",
    "\n",
    "#  parameters, forward = forward_prop_dropout(n_h, n_f, n_O, X, keep_prob)\n",
    "\n",
    "def backward_prop_dropout(X, Y, forward, parameters, keep_prob):\n",
    "    \n",
    "    '''\n",
    "    Description: This funcition performs backward propagation with drop out in mind.\n",
    "    \n",
    "    input - \n",
    "    X - training samples\n",
    "    Y - output value\n",
    "    forward - forward prop results\n",
    "    parameters - parameters of theneural net\n",
    "    keep_prob - keep probability \n",
    "    \n",
    "    Output - \n",
    "    \n",
    "    gradients - the calculated gradients\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    #Get sample size\n",
    "    sample_size = X.shape[1]\n",
    "    \n",
    "    #Get parameters \n",
    "    Z1 = forward['Z1']\n",
    "    A = forward['A']\n",
    "    Z2 = forward['Z2']\n",
    "    Y_out = forward['Y_out']\n",
    "    D = forward['D']\n",
    "    \n",
    "    \n",
    "    #Calculate gradients for output layer\n",
    "    dZ2= Y_out - Y\n",
    "    dW2 = (1/sample_size) * np.dot(dZ2, A.T)\n",
    "    db2 = (1/sample_size) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    \n",
    "    #calculate gradients for hidden layer with drop out in mind\n",
    "    \n",
    "    '''\n",
    "    Simply, the gradient calculation is essentially the same as without drop out except that we will perform\n",
    "    all the drop out operations we performed for the parameters in the forward prop for their gradients. \n",
    "    '''\n",
    "    \n",
    "    #calculate gradient for dA\n",
    "    dA = np.dot(parameters['W2'].T, dZ2) * (1 - np.power(A, 2))\n",
    "    \n",
    "    #Multiply by drop out binary \n",
    "    dA = dA * D\n",
    "    \n",
    "    # divide by the keep probability so as to scale\n",
    "    dA = dA / keep_prob\n",
    "    \n",
    "    #calculate other gradients\n",
    "    dW1 = (1/sample_size) * np.dot(dZ1, X.T)\n",
    "    db1 = (1/sample_size) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    # store calculated gradients\n",
    "    \n",
    "    gradients = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There you have it â€“ the implementation of drop out regularization from scratch in forward propagation and backward propagation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
