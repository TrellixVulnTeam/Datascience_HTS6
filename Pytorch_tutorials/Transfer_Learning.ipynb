{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /home/akshay/.torch/models/vgg16-397923af.pth\n",
      "100%|██████████| 553433881/553433881 [00:36<00:00, 15129328.53it/s]\n"
     ]
    }
   ],
   "source": [
    "#import models from torchvision\n",
    "\n",
    "#https://medium.com/@tsakunelsonz/loading-and-training-a-neural-network-with-custom-dataset-via-transfer-learning-in-pytorch-8e672933469\n",
    "from torchvision import models\n",
    "\n",
    "#build the pretrained model (vgg16 in this case)\n",
    "model = models.vgg16(pretrained = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We downloaded VGG16 model which is pre-trained from torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import OrderedDicted to corectly align the network layers\n",
    "#import nn you use activation and dropout features\n",
    "\n",
    "from collections import OrderedDict\n",
    "from torch import nn\n",
    "\n",
    "#create classifier\n",
    "classifier=nn.Sequential(OrderedDict([('fc1', nn.Linear(25088, 512)),\n",
    "                           ('relu', nn.ReLU()), \n",
    "                           ('dropout', nn.Dropout(p=0.337)),\n",
    "                           ('fc2', nn.Linear(512, 102)),\n",
    "                           ('output', nn.LogSoftmax(dim=1))\n",
    "                             ]))\n",
    "\n",
    "#replace the model's classifier with this new classifier \n",
    "#transfer learning connection applied here\n",
    "#model.classifier = classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two things worth noting here: The input size (25088 in this case) should be equivalent to that specified by the network, and the output size (102) should be  equivalent to the number of all the classes represented by the dataset. If we see the model has many parts like for feature, classification etc. We are modifying classifier so that it fits our dataset it terms of input and output size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model\n",
    "model.classifier = classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the network. This is done by first defining the loss function (Cross Entropy Loss is generally used) and the network optimizer (Stochastic Gradient Descent [SGD] in this case) with respective parameters:\n",
    "\n",
    "The below code snippet sets the learning rate to 0.005 (the step size taken by a model to minimize loss and update weights with the goal to improve predictive accuracy) and momentum of 0.5 (the amount by which a model can easily bump off any local minima during the gradient descent process while searching for the global minimum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import optimizer for \n",
    "from torch import optim\n",
    "\n",
    "#define criteria and optimizer\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.005, momentum = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network can now be trained, one sample at a time, throughout the whole sample size; the process is repeated this time with updated weights, yielding better accuracies. The latter is done iteratively for a defined number of times. This process is cheaply and accurately achieved with two defined functions and an iterative loop from zero to the number of epochs times as follows:\n",
    "\n",
    "    The first is the training function, which takes in the defined model, the dataset (training Loader) and the loss criterion, then returns the loss and accuracy achieved for each epoch as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define training function\n",
    "def train (model, loader, criterion, gpu):\n",
    "    model.train()\n",
    "    current_loss = 0\n",
    "    current_correct = 0\n",
    "    for train, y_train in iter(loader):\n",
    "        if gpu:\n",
    "            train, y_train = train.to('cuda'), y_train.to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(train)\n",
    "        _, preds = torch.max(output,1)\n",
    "        loss = criterion(output, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        current_loss += loss.item()*train.size(0)\n",
    "        current_correct += torch.sum(preds == y_train.data)\n",
    "    epoch_loss = current_loss / len(trainLoader.dataset)\n",
    "    epoch_acc = current_correct.double() / len(trainLoader.dataset)\n",
    "        \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second is the validation function, which takes in the defined model, the dataset (validation Loader) and the loss criterion; it also returns the loss and accuracy for each epoch as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define validation function\n",
    "def validation (model, loader, criterion, gpu):\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    valid_correct = 0\n",
    "    for valid, y_valid in iter(loader):\n",
    "        if gpu:\n",
    "            valid, y_valid = valid.to('cuda'), y_valid.to('cuda')\n",
    "        output = model.forward(valid)\n",
    "        valid_loss += criterion(output, y_valid).item()*valid.size(0)\n",
    "        equal = (output.max(dim=1)[1] == y_valid.data)\n",
    "        valid_correct += torch.sum(equal)#type(torch.FloatTensor)\n",
    "    \n",
    "    epoch_loss = valid_loss / len(validLoader.dataset)\n",
    "    epoch_acc = valid_correct.double() / len(validLoader.dataset)\n",
    "    \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: Two things worth noting here. The training function switches the model to training mode and initializes the optimizer gradients to zero, before updating. On the other hand, the validation function instead switches the model to evaluation mode and receives the updated gradients from the training function, for evaluation.\n",
    "\n",
    "The last step here is to combine both functions in a loop for the number of epochs times (20 in this case). During each iteration, the loss and accuracy produced by each function are displayed. Following is the code snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'arg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-4b2c5453d69d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#send model to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'arg' is not defined"
     ]
    }
   ],
   "source": [
    "#Initialize training params  \n",
    "#freeze gradient parameters in pretrained model\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.require_grad = False\n",
    "\n",
    "#train and validate\n",
    "epochs = 10  \n",
    "epoch = 0\n",
    "arg.gpu=False\n",
    "\n",
    "#send model to GPU\n",
    "if args.gpu:\n",
    "    model.to('cuda')\n",
    "    \n",
    "for e in range(epochs):\n",
    "    epoch +=1\n",
    "    print(epoch)\n",
    "    with torch.set_grad_enabled(True):\n",
    "        epoch_train_loss, epoch_train_acc = train(model,trainLoader, criteria, args.gpu)\n",
    "        print(\"Epoch: {} Train Loss : {:.4f}  Train Accuracy: {:.4f}\".format(epoch,epoch_train_loss,epoch_train_acc))\n",
    "\n",
    "with torch.no_grad():\n",
    "        epoch_val_loss, epoch_val_acc = validation(model, validLoader, criteria, args.gpu)\n",
    "        print(\"Epoch: {} Validation Loss : {:.4f}  Validation Accuracy {:.4f}\".format(epoch,epoch_val_loss,epoch_val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: Two things worth noting here: Gradients must be turned true during the training phase, then turned off during the validation phase before calling the respective functions. Secondly, the validation loss should generally be lower compared to the training loss at the current epoch (iteration), and at the same time, the validation accuracy should be higher compared to the training accuracy. Combined with the latter observation, if the loss is constantly decreasing while the accuracy constantly increases, then you are in the light of producing an accurate network for your custom dataset.\n",
    "\n",
    "Having trained and evaluated our Network with a good accuracy, we are more than ready to test the network with new data of related classes.\n",
    "Testing the Modified Network\n",
    "\n",
    "It is good practice to always test the trained network on test data which the network has never seen either in training nor validation. This gives a good estimate for the model’s performance on completely new inputs. Here, we pass the test images through the network and measure the accuracy just as in the validation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-cd20ad7c9270>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#iterating for each sample in the test dataset once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#Calculate the class probabilities (softmax) for img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'testLoader' is not defined"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total = 0\n",
    "correct = 0 \n",
    "count = 0\n",
    "\n",
    "#iterating for each sample in the test dataset once\n",
    "for test, y_test in iter(testLoader):\n",
    "    test, y_test = test.to('cuda'), y_test.to('cuda')\n",
    "#Calculate the class probabilities (softmax) for img\n",
    "    with torch.no_grad():\n",
    "        output = model.forward(test)\n",
    "        ps = torch.exp(output)\n",
    "        _, predicted = torch.max(output.data,1)\n",
    "        total += y_test.size(0)\n",
    "        correct += (predicted == y_test).sum().item() \n",
    "        count += 1\n",
    "        print(\"Accuracy of network on test images is ... {:.4f}....count: {}\".format(100*correct/total,  count ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the checkpoint and save every sensitive information starting #from the model state dictionary, model criterion, optimizer, to the #number of epochs\n",
    "checkpoint = {'model_state': model.state_dict(),\n",
    "              'criterion_state': criteria.state_dict(),\n",
    "              'optimizer_state': optimizer.state_dict(),\n",
    "              'class_to_idx': train_datasets.class_to_idx,\n",
    "              'epochs': epochs,\n",
    "              'Best train loss': epoch_train_loss,\n",
    "              'Best train accuracy': epoch_train_accuracy,\n",
    "              'Best Validation loss': epoch_val_loss,\n",
    "              'Best Validation accuracy': epoch_val_acc}\n",
    "\n",
    "torch.save(checkpoint, args.checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
