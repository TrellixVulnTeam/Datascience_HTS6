{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing data and outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real world data often needs to be cleaned before it can be used in predictive modelling. Data cleaning and preparation is incredibly important – if missing values or outliers are not handled, they can lead to the incorrect analyses of behaviour between variables.\n",
    "\n",
    "We will discuss how to check for and fix these issues in this step, using the popular dataset `titanic.csv`. This dataset is commonly used by beginners to data science and machine learning to explore whether certain traits of passengers predicted whether they survived the Titanic sinking.\n",
    "\n",
    "### Checking for missing values\n",
    "\n",
    "There are many methods to check for missing values, including built-in methods in `pandas`. Let’s import  the `titanic` dataset and take a look at the statistical summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `titanic` as pandas dataframe\n",
    "titanic = pd.read_csv('titanic.csv')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shape and statistical summary of dataset\n",
    "print(titanic.shape)\n",
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 891 rows in this dataset, but the count for the column `Age` is only 714! \n",
    "\n",
    "We can easily check which rows have `null` values by using `isnull()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "titanic['Age'].isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a series indicating `True` if the row is `null`, and `False` if the row is `not null`. If we wanted to get the total number of `null` rows, we can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic['Age'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This checks out! 177 `null` rows and 714 `not null` rows gives us a total of 891. \n",
    "\n",
    "The `describe` method we used earlier only shows columns with numeric values…what if there are columns with categorical values with `null`s? We can check the entire dataframe as a whole, in case other columns also have `null` values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we’ve identified missing data, how do we treat them? There are two basic approaches to handle missing values – **deletion** and **imputation**. Deleting means to delete all rows with one or more missing values. Imputation means replacing the missing values with other values based on the context of the data. Let’s go through some examples!\n",
    "\n",
    "### Deleting missing values\n",
    "\n",
    "You can choose to either delete an entire row or an entire column, depending on the context. In our example, if we believe `Age` is an important predictor in whether a passenger survived the Titanic shipwreck, we would definitely not want to delete the entire column. \n",
    "\n",
    "We may also not want to delete all the rows with missing data, since this will remove a large portion of our data (177/714 = 25% of our data!).\n",
    "\n",
    "However, there are only 2 values missing in the column `Embarked`. We may be able to delete these two rows without decreasing our ability to create an effective predictive model too much. Let’s give it a go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete any row with null values in the `Embarked` column\n",
    "titanic = titanic.dropna(subset=['Embarked'], axis=0)\n",
    "\n",
    "# Check that rows with null values in `Embarked` were dropped\n",
    "titanic['Embarked'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dropna()` method can also be used to:\n",
    "- drop entire columns (by changing `axis` to `1`),\n",
    "- drop null columns or rows from the entire dataset (by not specifying `subset`), and \n",
    "- drop columns or rows where all or any values are `null` (by adding `how='all'` or `how='any'` inside the parentheses). \n",
    "\n",
    "### Imputing missing values\n",
    "\n",
    "Imputation is the method of adding/replacing missing values with other values, such as `0` or the mean or median of the other `not null` values. The `fillna()` method is used to do this.\n",
    "\n",
    "You can fill in every missing value in the entire dataset with the same value (e.g `titanic.fillna(0)` replaces all `null` with zero), or you can specify a column to fill in the missing values for just that column (e.g. `titanic[‘Ages’].fillna(0)`). \n",
    "\n",
    "Deciding what to fill in your values with depends on your data. Replacing `null` values in the `Ages` column with zero would not make much sense since this would make the ages of 177 people zero. Instead, let’s impute the missing values in `Ages` with the mean age of all the other passengers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace any null values in `Age` with the mean age\n",
    "mean_age = titanic['Age'].mean()\n",
    "titanic['Age'] = titanic['Age'].fillna(mean_age)\n",
    "\n",
    "# Check that rows with null values in `Age` were imputed\n",
    "titanic['Age'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! There are no more missing values in `Age`! \n",
    "\n",
    "We can also fill in missing values with strings – let’s fill in the `null`s in `Cabin` with the string “missing”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace any null values in `Cabin` with the the string \"missing\"\n",
    "mean_age = titanic['Age'].mean()\n",
    "titanic['Cabin'] = titanic['Cabin'].fillna(\"missing\")\n",
    "\n",
    "# Check that rows with null values in `Cabin` were imputed\n",
    "titanic['Cabin'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset for null values\n",
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for outliers\n",
    "\n",
    "Outliers are anomalous values that can distort predictive models and reduce their efficacy. Luckily they are quite easy to detect. The easiest method is to visualise the variables as **boxplots** (if you are not familiar with boxplots, [here](https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51) is a great explanation).\n",
    "\n",
    "For our `titanic` example, we used the `describe` method earlier to display statistical summaries for qualitative variables. Taking a closer look, we see that the max value for `Fare` (512.33)  is many standard deviations greater than the `mean` (32.20). This is a good indicator that there may be outliers in our data. Let’s create our boxplot to check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boxplot using matplotlib\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.boxplot(titanic['Fare'], vert=False)   # vert turns the plot horizontal or vertical\n",
    "plt.xlabel('Fare')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The red line shows the mean of `Fare`. Generally speaking, points less than the first quartile (left whisker) and points higher than the third quartile (right whisker) are considered outliers.\n",
    "\n",
    "As you see, there are quite a few values outside the third quartile, with one value in particular much higher than the rest. This could be a result of error in data collection, or some people truly did pay an exorbitant fare…either way, this outlier could distort any prediction model we try to implement. \n",
    "\n",
    "### Removing outliers\n",
    "\n",
    "We may decide to remove just extreme outlier from our data, or we may remove all values higher than the third quartile. \n",
    "\n",
    "To remove just the extreme outlier, let’s first find out which rows these values are in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the index of the rows with the max fare value\n",
    "max_fare = max(titanic['Fare'])\n",
    "max_fare_idx = titanic.index[titanic['Fare'] == max_fare].tolist()\n",
    "\n",
    "print(f'Passengers in rows {max_fare_idx} paid the highest fare of ${max_fare}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like three passengers paid 512.33 for their tickets, even though the mean fare was only 32.20! \n",
    "\n",
    "Now we can remove these three rows from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic2 = titanic.drop(index=max_fare_idx)\n",
    "titanic2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our dataset is now 886 rows after removing these three rows with extreme `Fare` outliers. \n",
    "\n",
    "If we decided instead to remove all values greater than the third quartile, we can use a similar method but rather than filtering for `Fare == max(Fare)` we can filter for `Fare > Q3`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the index of rows with fares greater than the third quartile\n",
    "Q3 = np.percentile(titanic['Fare'], 75)\n",
    "greaterthanQ3_idx = titanic.index[titanic['Fare'] > Q3].tolist()\n",
    "\n",
    "print(f'{len(greaterthanQ3_idx)} passengers paid a fare greater than ${Q3}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic3 = titanic.drop(index=greaterthanQ3_idx)\n",
    "titanic3.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the `Fare` column has no outliers! Keep in mind, however, that this is not a fool-proof method. As you see from the statistical summary of the new dataset, there are now only 669 records – we’ve removed 220 rows! It is necessary to check whether removing all these outliers improves your linear regression model, or if removing all this data actually decreases efficacy.\n",
    "\n",
    "Next, we will explore another issue we may run into – how to handle qualitative variables. Go back to the notebook directory in Jupyter by pressing `File` > `Open…` in the toolbar at the top, then open the notebook called `3.2 Categorical variables.ipynb`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
