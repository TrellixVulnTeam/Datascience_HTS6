In this lesson, we will cover the following:

    Reading and plotting audio data (read_plot.py)
    Transforming audio signals into the frequency domain (freq_transform.py)
    Generating audio signals with custom parameters(generate.py)
    Synthesizing music (synthesize_music.py)
    Extracting frequency domain features (extract_freq_features.py)
    Building Hidden Markov Models (speech_recognizer.py)
    Building a speech recognizer	

Speech recognition refers to the process of recognizing and understanding spoken language. Input comes in the form of audio data, and the speech recognizers will process this data to extract meaningful information from it. This has a lot of practical uses, such as voice controlled devices, transcription of spoken language into words, security systems, and so on.

Speech signals are very versatile in nature. There are many variations of speech in the same language. There are different elements to speech, such as language, emotion, tone, noise, accent, and so on. It's difficult to rigidly define a set of rules that can constitute speech. Even with all these variations, humans are really good at understanding all of this with relative ease. Hence, we need machines to understand speech in the same way.

Over the last couple of decades, researchers have worked on various aspects of speech, such as identifying the speaker, understanding words, recognizing accents, translating speech, and so on. Among all these tasks, automatic speech recognition has been the focal point of attention for many researchers. In this lesson, we will learn how to build a speech recognizer.

Let's take a look at how to read an audio file and visualize the signal. This will be a good starting point, and it will give us a good understanding about the basic structure of audio signals. Before we start, we need to understand that audio files are digitized versions of actual audio signals. Actual audio signals are complex continuous-valued waves. In order to save a digital version, we sample the signal and convert it into numbers. For example, speech is commonly sampled at 44100 Hz. This means that each second of the signal is broken down into 44100 parts, and the values at these timestamps are stored. In other words, you store a value every 1/44100 seconds. As the sampling rate is high, we feel that the signal is continuous when we listen to it on our media players.



Fourier Transforms: How to do it

Audio signals consist of a complex mixture of sine waves of different frequencies, amplitudes, and phases. Sine waves are also referred to as sinusoids. There is a lot of information that is hidden in the frequency content of an audio signal. In fact, an audio signal is heavily characterized by its frequency content. The whole world of speech and music is based on this fact. Before you proceed further, you will need some knowledge about Fourier transforms. A quick refresher can be found at here. (http://www.thefouriertransform.com/) Now, let's take a look at how to transform an audio signal into the frequency domain.

Let's apply the Fourier transform. The Fourier transform signal is mirrored along the center, so we just need to take the first half of the transformed signal. Our end goal is to extract the power signal. So, we square the values in the signal in preparation for this:

We can use NumPy to generate audio signals. As we discussed earlier, audio signals are complex mixtures of sinusoids. So, we will keep this in mind when we generate our own audio signal.

Now that we know how to generate audio, let's use this principle to synthesize some music. You can check out this link.(https://pages.mtu.edu/~suits/notefreqs.html) This link lists various notes, such as A, G, D, and so on, along with their corresponding frequencies. We will use this to generate some simple music.

Mel Frequency Cepstral Coefficients

We discussed earlier how to convert a signal into the frequency domain. In most modern speech recognition systems, people use frequency-domain features. After you convert a signal into the frequency domain, you need to convert it into a usable form. Mel Frequency Cepstral Coefficients (MFCC) is a good way to do this.

MFCC takes the power spectrum of a signal and then uses a combination of filter banks and discrete cosine transform to extract features. If you need a quick refresher, you can do that here. (This lab requires the python_speech_features package, which has already been installed. You can read more about it here).

Let's take a look at how to extract MFCC features.

We are now ready to discuss speech recognition. We will use Hidden Markov Models (HMMs) to perform speech recognition. HMMs are great at modeling time series data. As an audio signal is a time series signal, HMMs perfectly suit our needs. An HMM is a model that represents probability distributions over sequences of observations. We assume that the outputs are generated by hidden states. So, our goal is to find these hidden states so that we can model the signal. You can learn more about it here. (This lab requires the hmmlearn package, which has already been installed. You can read more about it here). https://hmmlearn.readthedocs.io/en/latest/

Let's take a look at how to build HMMs.

We need a database of speech files to build our speech recognizer. We will use the database available here. (https://code.google.com/archive/p/hmm-speech-recognition/downloads) These data have been provided in the data folder in this lab. This database contains seven different words, where each word has 15 audio files associated with it. This is a small dataset, but this is sufficient to understand how to build a speech recognizer that can recognize seven different words. We need to build an HMM model for each class. When we want to identify the word in a new input file, we need to run all the models on this file and pick the one with the best score. We will use the HMM class that we built in the previous step.
